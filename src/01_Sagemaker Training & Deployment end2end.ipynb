{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c095987-0137-4314-8327-3be747780a7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Human Activity Recognition - AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894db4cb-5b82-4573-8cfe-7b16358b696b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem Statement\n",
    "Let’s open the notebook “HAR Model training notebook”. The problem statement for this notebook is: Deploying the Human Activity Recognition problem using the Level 1 MLOps architecture, the aim is to enhance the experience of Blackmi's health app by overcoming the problems faced in the level 0 architecture. Utilising the Human Activity Recognition dataset, we will construct a machine-learning model along with the ML pipelines to categorise user activities for the real-time health alerts using AWS sagemaker studio. Here we will also be monitoring the model performance and deploy the model using different deployment techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57df07-1b14-4dc2-9860-27cdf0ef8c93",
   "metadata": {},
   "source": [
    "### Approach \n",
    "In this notebook we will be building the level 1 architecture of MLOps, and our major focus would be on creating ML pipeline, model monitoring and model deployment. The major take away for this lesson is to learn:\n",
    "\n",
    "1. Feature engineering with the amazon sagemaker processing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1123b0-e1a4-4d15-b0eb-0ff261b8f05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries \n",
    "# Importing pandas and numpy for data preprocessing. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Boto3 is used for launching the EC2 instances and manipulating s3 buckets.\n",
    "import boto3\n",
    "# Sagemaker is imported for building, training and deploying machine learning models.\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e63645c-aed6-4890-9d73-93d1b5f6e98b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ap-south-1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising new sagemaker session as \"sess\".\n",
    "sess = sagemaker.Session()\n",
    "# Check for necessary permission needed for training and deploying models. \n",
    "role = sagemaker.get_execution_role()\n",
    "# To understand where this session is configured to operate.\n",
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6895d1d8-9c61-4e97-a154-280490992c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bucket variable is used for storing the location of the bucket\n",
    "bucket = 'sagemaker-studio-009676737623-l4vs7j0o0ib'\n",
    "# Assigning the prefix variable \n",
    "prefix = 'mlops-level1-data'\n",
    "# input_source variable is used for storing the location of the dataset\n",
    "input_source = 's3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/train_data.gzip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79901750-55f7-42e2-b3df-f2d81e8d330c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_df = pd.read_parquet(input_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a51043-8511-4e58-a0d4-535dba260a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 563)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ca4d61-958e-468f-8e0e-0e501444cc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_df_csv = input_df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806539ab-98dd-4a18-9829-c8474478c1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_df_csv.to_csv('s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/train_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eab099-4c31-462b-aeaf-46d5bcc9c1d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering with Amazon SageMaker Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c741aa1c-4236-4157-a207-d1b02e80ba17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "def _parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Data, model, and output directories\n",
    "    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n",
    "    parser.add_argument('--filepath', type=str, default='/opt/ml/processing/input/')\n",
    "    parser.add_argument('--filename', type=str, default='train_data.gzip')\n",
    "    parser.add_argument('--outputpath', type=str, default='/opt/ml/processing/output/')\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_top_k_features(X, Y, k):\n",
    "        clf = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = clf.fit(X, Y)\n",
    "        feature_df = pd.DataFrame(\n",
    "            data=(X.columns, clf.feature_importances_)\n",
    "        ).T.sort_values(by=1, ascending=False)\n",
    "        cols = feature_df.head(k)[0].values\n",
    "        return cols\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "    # Load data\n",
    "    path = os.path.join(args.filepath,args.filename)\n",
    "    print(path)\n",
    "    \n",
    "    # Reading the dataset and performing label encoding \n",
    "    df = pd.read_parquet(os.path.join(args.filepath,args.filename))\n",
    "    le = LabelEncoder()\n",
    "    df['Activity'] = le.fit_transform(df['Activity'])\n",
    "    df.drop(['date_time'],axis =1 ,inplace = True)\n",
    "\n",
    "    # Assignining the indepeneded and depended variable \n",
    "    X = df.drop(['Activity'], axis =1)\n",
    "    Y = df['Activity']\n",
    "    \n",
    "    # Extracting top 12 important feature and filtering the dataset\n",
    "    k =12\n",
    "    final_cols = get_top_k_features(X, Y, k)\n",
    "    final_cols = np.append(final_cols,np.array(['Activity']))\n",
    "    df = df[final_cols]\n",
    "    \n",
    "    # Train, test, validation split\n",
    "    # Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "    train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=42), [int(0.8 * len(df)), int(0.9 * len(df))])  \n",
    "    \n",
    "    # Storing of train, validation and test datasets \n",
    "    pd.concat([train_data['Activity'], train_data.drop(['Activity'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'train/train.csv'), index=False, header=False)\n",
    "    pd.concat([validation_data['Activity'], validation_data.drop(['Activity'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'validation/validation.csv'), index=False, header=False)\n",
    "    test_data[['Activity']].to_csv(os.path.join(args.outputpath, 'test/test_y.csv'), index=False, header=False)\n",
    "    test_data.drop(['Activity'], axis=1).to_csv(os.path.join(args.outputpath, 'test/test_x.csv'), index=False, header=False)\n",
    "    \n",
    "    ## Save Features columns\n",
    "    dump(final_cols, os.path.join(args.outputpath, 'feature/feature.joblib'))\n",
    "    ## Save Encoder\n",
    "    dump(le, os.path.join(args.outputpath, 'feature/encoder.joblib'))\n",
    "    \n",
    "    print(\"## Processing complete. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5522538f-57f2-457a-8936-b11ca432384c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test\"\n",
    "feature_path = f\"s3://{bucket}/{prefix}/feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa725f3b-c253-4c84-afd5-7a24e3a7fa81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sklearn-ml-train-2023-09-26-05-33-44-480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34m/opt/ml/processing/input/train_data.gzip\u001b[0m\n",
      "\u001b[34m## Processing complete. Exiting.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary library for data processing \n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=get_execution_role(),\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1, \n",
    "    base_job_name='sklearn-ml-train'\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=input_source, \n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\", \n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=train_path,\n",
    "        ),\n",
    "        ProcessingOutput(output_name=\"validation_data\", source=\"/opt/ml/processing/output/validation\", destination=validation_path),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/output/test\", destination=test_path),\n",
    "        ProcessingOutput(output_name=\"feature_data\", source=\"/opt/ml/processing/output/feature\", destination=feature_path)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03df72-150c-403f-a8ec-045800786489",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c264cd4-10d5-4847-b508-296ea7ee98d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.TrainingInput(s3_data=train_path.format(bucket, prefix), \n",
    "                                                    content_type='csv')\n",
    "s3_input_validation = sagemaker.TrainingInput(s3_data=validation_path.format(bucket, prefix),\n",
    "                                                     content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1449514a-1c38-489c-a363-e205d4adc54d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sklearn-train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sklearn-train.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load\n",
    "import pandas as pd, numpy as np, os, argparse\n",
    "\n",
    "## predict\n",
    "\n",
    "# inference function - tells SageMaker how to load the model\n",
    "def model_fn(model_dir):\n",
    "    clf = load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), \n",
    "                         header=None)\n",
    "\n",
    "        if len(df.columns) == len(feature_columns_names) + 1:\n",
    "            # This is a labelled example, includes the ring label\n",
    "            df.columns = feature_columns_names + [label_column]\n",
    "        elif len(df.columns) == len(feature_columns_names):\n",
    "            # This is an unlabelled example.\n",
    "            df.columns = feature_columns_names\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({\"features\": row})\n",
    "\n",
    "        json_output = {\"instances\": instances}\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), accept, mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), accept, mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))\n",
    "        \n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "\n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "\n",
    "    The output is returned in the following order:\n",
    "\n",
    "        rest of features either one hot encoded or standardized\n",
    "    \"\"\"\n",
    "    features = model.transform(input_data)\n",
    "\n",
    "    if label_column in input_data:\n",
    "        # Return the label (as the first column) and the set of features.\n",
    "        return np.insert(features, 0, input_data[label_column], axis=1)\n",
    "    else:\n",
    "        # Return only the set of features\n",
    "        return features        \n",
    "        \n",
    "# Argument parser\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=3)\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test.csv\")\n",
    "    # Parse the arguments\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "# Main Training Loop\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "    # Load the dataset\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "    # Separate X and y\n",
    "    X_train, y_train = train_df.drop(train_df.columns[0], axis=1), train_df[train_df.columns[0]]\n",
    "    X_test, y_test = test_df.drop(test_df.columns[0], axis=1), test_df[test_df.columns[0]]\n",
    "    # Define the model and train it\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=args.n_estimators, n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    # Evaluate the model performances\n",
    "    print(f'Model Accuracy: {accuracy_score(y_test, model.predict(X_test))}')\n",
    "    dump(model, os.path.join(args.model_dir, 'model.joblib'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4fd364-fe2d-4a93-9305-3035093bcfee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "WARNING:sagemaker.interactive_apps.base_interactive_app:NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n",
      "INFO:sagemaker:Creating training-job with name: rf-scikit-2023-09-26-05-39-19-263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-09-26 05:39:20 Starting - Starting the training job...\n",
      "2023-09-26 05:39:37 Starting - Preparing the instances for training...\n",
      "2023-09-26 05:40:19 Downloading - Downloading input data......\n",
      "2023-09-26 05:41:10 Training - Training image download completed. Training in progress..\u001b[34m2023-09-26 05:41:11,285 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2023-09-26 05:41:11,288 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 05:41:11,330 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-26 05:41:11,480 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 05:41:11,491 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 05:41:11,501 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-26 05:41:11,509 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"min-samples-leaf\": 3,\n",
      "        \"n-estimators\": 120,\n",
      "        \"test-file\": \"validation.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"rf-scikit-2023-09-26-05-39-19-263\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-009676737623/rf-scikit-2023-09-26-05-39-19-263/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn-train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"min-samples-leaf\":3,\"n-estimators\":120,\"test-file\":\"validation.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-009676737623/rf-scikit-2023-09-26-05-39-19-263/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"min-samples-leaf\":3,\"n-estimators\":120,\"test-file\":\"validation.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"rf-scikit-2023-09-26-05-39-19-263\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-009676737623/rf-scikit-2023-09-26-05-39-19-263/source/sourcedir.tar.gz\",\"module_name\":\"sklearn-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--min-samples-leaf\",\"3\",\"--n-estimators\",\"120\",\"--test-file\",\"validation.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MIN-SAMPLES-LEAF=3\u001b[0m\n",
      "\u001b[34mSM_HP_N-ESTIMATORS=120\u001b[0m\n",
      "\u001b[34mSM_HP_TEST-FILE=validation.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python sklearn-train.py --min-samples-leaf 3 --n-estimators 120 --test-file validation.csv\u001b[0m\n",
      "\u001b[34mModel Accuracy: 0.8297829782978298\u001b[0m\n",
      "\u001b[34m2023-09-26 05:41:25,480 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-26 05:41:41 Uploading - Uploading generated training model\n",
      "2023-09-26 05:41:41 Completed - Training job completed\n",
      "Training seconds: 81\n",
      "Billable seconds: 81\n"
     ]
    }
   ],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "# Define the Estimator from SageMaker (Script Mode)\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"sklearn-train.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"rf-scikit\",\n",
    "    metric_definitions=[{\"Name\": \"Accuracy\", \"Regex\": \"Accuracy: ([0-9.]+).*$\"}],\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": 120,\n",
    "        \"min-samples-leaf\": 3,\n",
    "        \"test-file\": \"validation.csv\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Train the model (~5 minutes)\n",
    "sklearn_estimator.fit({\"train\": s3_input_train, \"test\": s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f32d72-bfa5-48a3-8372-9085235287dc",
   "metadata": {},
   "source": [
    "## Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "877aa689-0567-4489-8229-92bdac4f0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Hyperparameter Tuner\n",
    "from sagemaker.tuner import IntegerParameter\n",
    "\n",
    "# Define exploration boundaries\n",
    "hyperparameter_ranges = {\n",
    "    \"n-estimators\": IntegerParameter(100, 200),\n",
    "    \"min-samples-leaf\": IntegerParameter(2, 6)\n",
    "}\n",
    "\n",
    "Optimizer = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=sklearn_estimator,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name=\"RF-tuner\",\n",
    "    objective_type=\"Maximize\",\n",
    "    objective_metric_name=\"Accuracy\",\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"Accuracy\", \"Regex\": \"Accuracy: ([0-9.]+).*$\"}\n",
    "    ],  # extract tracked metric from logs with regexp\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ac8dc2e-fd35-49c4-936d-4c0d3fdc4d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: RF-tuner-230925-0520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "....................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "Optimizer.fit({\"train\": s3_input_train, \"test\": s3_input_validation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6daf9cd2-e16f-44a0-a5f7-1f5c6e0031ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min-samples-leaf</th>\n",
       "      <th>n-estimators</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>RF-tuner-230925-0520-010-93682ca2</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.830283</td>\n",
       "      <td>2023-09-25 05:28:10+00:00</td>\n",
       "      <td>2023-09-25 05:29:16+00:00</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>RF-tuner-230925-0520-009-19ef07a7</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.829683</td>\n",
       "      <td>2023-09-25 05:27:42+00:00</td>\n",
       "      <td>2023-09-25 05:28:48+00:00</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>RF-tuner-230925-0520-008-850fc1cd</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.830083</td>\n",
       "      <td>2023-09-25 05:26:51+00:00</td>\n",
       "      <td>2023-09-25 05:27:57+00:00</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>RF-tuner-230925-0520-007-3d7fd14e</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.829483</td>\n",
       "      <td>2023-09-25 05:26:30+00:00</td>\n",
       "      <td>2023-09-25 05:27:27+00:00</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>RF-tuner-230925-0520-006-5bd31f0d</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.831383</td>\n",
       "      <td>2023-09-25 05:25:05+00:00</td>\n",
       "      <td>2023-09-25 05:26:11+00:00</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min-samples-leaf  n-estimators                    TrainingJobName  \\\n",
       "0               6.0         178.0  RF-tuner-230925-0520-010-93682ca2   \n",
       "1               6.0         183.0  RF-tuner-230925-0520-009-19ef07a7   \n",
       "2               6.0         186.0  RF-tuner-230925-0520-008-850fc1cd   \n",
       "3               6.0         102.0  RF-tuner-230925-0520-007-3d7fd14e   \n",
       "4               6.0         193.0  RF-tuner-230925-0520-006-5bd31f0d   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "0         Completed             0.830283 2023-09-25 05:28:10+00:00   \n",
       "1         Completed             0.829683 2023-09-25 05:27:42+00:00   \n",
       "2         Completed             0.830083 2023-09-25 05:26:51+00:00   \n",
       "3         Completed             0.829483 2023-09-25 05:26:30+00:00   \n",
       "4         Completed             0.831383 2023-09-25 05:25:05+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds  \n",
       "0 2023-09-25 05:29:16+00:00                        66.0  \n",
       "1 2023-09-25 05:28:48+00:00                        66.0  \n",
       "2 2023-09-25 05:27:57+00:00                        66.0  \n",
       "3 2023-09-25 05:27:27+00:00                        57.0  \n",
       "4 2023-09-25 05:26:11+00:00                        66.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get tuner results in a df\n",
    "results = Optimizer.analytics().dataframe()\n",
    "while results.empty:\n",
    "    time.sleep(1)\n",
    "    results = Optimizer.analytics().dataframe()\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43191136-3a91-4cfa-877f-aacc453e82f5",
   "metadata": {},
   "source": [
    "## Sagemaker XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b2213a4-c7cd-4f30-b369-937e80160b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.interactive_apps.base_interactive_app:NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-09-26-05-47-34-821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-26 05:47:34 Starting - Starting the training job...\n",
      "2023-09-26 05:47:51 Starting - Preparing the instances for training......\n",
      "2023-09-26 05:48:56 Downloading - Downloading input data\n",
      "2023-09-26 05:48:56 Training - Downloading the training image...\n",
      "2023-09-26 05:49:31 Training - Training image download completed. Training in progress....\u001b[34m[2023-09-26 05:49:50.393 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.416 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-2776727441156358946\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] creating symlink between Path /opt/ml/input/data/validation/validation.csv and destination /tmp/sagemaker_xgboost_input_data/validation.csv2134491415977569373\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Train matrix has 80000 rows and 12 columns\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Validation matrix has 10000 rows\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.939 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.940 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.941 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:50.941 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-09-26:05:49:50:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-mlogloss:1.42600#011validation-mlogloss:1.42603\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:51.283 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2023-09-26 05:49:51.285 ip-10-0-151-179.ap-south-1.compute.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-mlogloss:1.20841#011validation-mlogloss:1.20899\u001b[0m\n",
      "\u001b[34m[2]#011train-mlogloss:1.05531#011validation-mlogloss:1.05639\u001b[0m\n",
      "\u001b[34m[3]#011train-mlogloss:0.93999#011validation-mlogloss:0.94156\u001b[0m\n",
      "\u001b[34m[4]#011train-mlogloss:0.85060#011validation-mlogloss:0.85237\u001b[0m\n",
      "\u001b[34m[5]#011train-mlogloss:0.77795#011validation-mlogloss:0.78003\u001b[0m\n",
      "\u001b[34m[6]#011train-mlogloss:0.72004#011validation-mlogloss:0.72202\u001b[0m\n",
      "\u001b[34m[7]#011train-mlogloss:0.67251#011validation-mlogloss:0.67463\u001b[0m\n",
      "\u001b[34m[8]#011train-mlogloss:0.63398#011validation-mlogloss:0.63630\u001b[0m\n",
      "\u001b[34m[9]#011train-mlogloss:0.60156#011validation-mlogloss:0.60404\u001b[0m\n",
      "\u001b[34m[10]#011train-mlogloss:0.57472#011validation-mlogloss:0.57712\u001b[0m\n",
      "\u001b[34m[11]#011train-mlogloss:0.55184#011validation-mlogloss:0.55446\u001b[0m\n",
      "\u001b[34m[12]#011train-mlogloss:0.53273#011validation-mlogloss:0.53579\u001b[0m\n",
      "\u001b[34m[13]#011train-mlogloss:0.51628#011validation-mlogloss:0.51936\u001b[0m\n",
      "\u001b[34m[14]#011train-mlogloss:0.50273#011validation-mlogloss:0.50606\u001b[0m\n",
      "\u001b[34m[15]#011train-mlogloss:0.49084#011validation-mlogloss:0.49458\u001b[0m\n",
      "\u001b[34m[16]#011train-mlogloss:0.48069#011validation-mlogloss:0.48450\u001b[0m\n",
      "\u001b[34m[17]#011train-mlogloss:0.47197#011validation-mlogloss:0.47601\u001b[0m\n",
      "\u001b[34m[18]#011train-mlogloss:0.46406#011validation-mlogloss:0.46821\u001b[0m\n",
      "\u001b[34m[19]#011train-mlogloss:0.45724#011validation-mlogloss:0.46170\u001b[0m\n",
      "\u001b[34m[20]#011train-mlogloss:0.45065#011validation-mlogloss:0.45535\u001b[0m\n",
      "\u001b[34m[21]#011train-mlogloss:0.44522#011validation-mlogloss:0.45013\u001b[0m\n",
      "\u001b[34m[22]#011train-mlogloss:0.44062#011validation-mlogloss:0.44572\u001b[0m\n",
      "\u001b[34m[23]#011train-mlogloss:0.43665#011validation-mlogloss:0.44191\u001b[0m\n",
      "\u001b[34m[24]#011train-mlogloss:0.43303#011validation-mlogloss:0.43832\u001b[0m\n",
      "\u001b[34m[25]#011train-mlogloss:0.42955#011validation-mlogloss:0.43524\u001b[0m\n",
      "\u001b[34m[26]#011train-mlogloss:0.42656#011validation-mlogloss:0.43241\u001b[0m\n",
      "\u001b[34m[27]#011train-mlogloss:0.42355#011validation-mlogloss:0.42956\u001b[0m\n",
      "\u001b[34m[28]#011train-mlogloss:0.42119#011validation-mlogloss:0.42731\u001b[0m\n",
      "\u001b[34m[29]#011train-mlogloss:0.41845#011validation-mlogloss:0.42475\u001b[0m\n",
      "\u001b[34m[30]#011train-mlogloss:0.41653#011validation-mlogloss:0.42297\u001b[0m\n",
      "\u001b[34m[31]#011train-mlogloss:0.41464#011validation-mlogloss:0.42121\u001b[0m\n",
      "\u001b[34m[32]#011train-mlogloss:0.41288#011validation-mlogloss:0.41960\u001b[0m\n",
      "\u001b[34m[33]#011train-mlogloss:0.41136#011validation-mlogloss:0.41831\u001b[0m\n",
      "\u001b[34m[34]#011train-mlogloss:0.40975#011validation-mlogloss:0.41687\u001b[0m\n",
      "\u001b[34m[35]#011train-mlogloss:0.40808#011validation-mlogloss:0.41527\u001b[0m\n",
      "\u001b[34m[36]#011train-mlogloss:0.40689#011validation-mlogloss:0.41421\u001b[0m\n",
      "\u001b[34m[37]#011train-mlogloss:0.40585#011validation-mlogloss:0.41330\u001b[0m\n",
      "\u001b[34m[38]#011train-mlogloss:0.40484#011validation-mlogloss:0.41239\u001b[0m\n",
      "\u001b[34m[39]#011train-mlogloss:0.40334#011validation-mlogloss:0.41113\u001b[0m\n",
      "\u001b[34m[40]#011train-mlogloss:0.40231#011validation-mlogloss:0.41027\u001b[0m\n",
      "\u001b[34m[41]#011train-mlogloss:0.40125#011validation-mlogloss:0.40946\u001b[0m\n",
      "\u001b[34m[42]#011train-mlogloss:0.40036#011validation-mlogloss:0.40860\u001b[0m\n",
      "\u001b[34m[43]#011train-mlogloss:0.39943#011validation-mlogloss:0.40785\u001b[0m\n",
      "\u001b[34m[44]#011train-mlogloss:0.39875#011validation-mlogloss:0.40731\u001b[0m\n",
      "\u001b[34m[45]#011train-mlogloss:0.39800#011validation-mlogloss:0.40659\u001b[0m\n",
      "\u001b[34m[46]#011train-mlogloss:0.39733#011validation-mlogloss:0.40626\u001b[0m\n",
      "\u001b[34m[47]#011train-mlogloss:0.39675#011validation-mlogloss:0.40579\u001b[0m\n",
      "\u001b[34m[48]#011train-mlogloss:0.39595#011validation-mlogloss:0.40517\u001b[0m\n",
      "\u001b[34m[49]#011train-mlogloss:0.39511#011validation-mlogloss:0.40467\u001b[0m\n",
      "\n",
      "2023-09-26 05:50:23 Uploading - Uploading generated training model\n",
      "2023-09-26 05:50:23 Completed - Training job completed\n",
      "Training seconds: 102\n",
      "Billable seconds: 102\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"num_class\":6,\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"multi:softmax\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "#bucket = sagemaker.Session().default_bucket()\n",
    "#prefix = 'DEMO-xgboost-as-a-built-in-algo'\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'abalone-xgb-built-in-algo')\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.7-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgboost_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"text/csv\"#\"libsvm\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, 'validation'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "xgboost_estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd118d2c-7e22-4b73-ae4b-5381be53d282",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd2029-b56d-449d-a55a-aec7b29720c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(validation_path + '/validation.csv')\n",
    "validation_data_feature = validation_data.iloc[:,1:]\n",
    "validation_data_label = validation_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637788d5-9d4c-4a3c-ae98-82a31c4325db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6f444-a5ed-40a4-9ca8-fd244c1ee9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebe91d-3701-453b-8fa8-7724533e1f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67abcfa8-6342-45bd-b81e-d664df4f5fd3",
   "metadata": {},
   "source": [
    "## Hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3b76241-fe51-4c74-a510-38ea5834778e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "# Set to True to enable data capture\n",
    "enable_capture = True\n",
    "\n",
    "# Optional - Sampling percentage. Choose an integer value between 0 and 100\n",
    "sampling_percentage = 100\n",
    "# sampling_percentage = 30 # Example 30%\n",
    "\n",
    "# Optional - The S3 URI of stored captured-data location\n",
    "s3_capture_upload_path =\"s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/datacapture/\"\n",
    "\n",
    "# Specify either Input, Output or both. \n",
    "capture_modes = ['REQUEST','RESPONSE'] # In this example, we specify both\n",
    "\n",
    "# Configuration object passed in when deploying Models to SM endpoints\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture = enable_capture, \n",
    "    sampling_percentage = sampling_percentage, # Optional\n",
    "    destination_s3_uri = s3_capture_upload_path, # Optional\n",
    "    capture_options = [\"REQUEST\", \"RESPONSE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e120031b-1fc2-412d-b0c0-b8968782489e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName = Sklearn-Inference-endpoint-2023-09-26-0614\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "sklearn_endpoint_name =f\"Sklearn-Inference-endpoint-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"EndpointName =\", sklearn_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820acd8-2b5d-4b42-b32c-96302ec90f48",
   "metadata": {},
   "source": [
    "### Deploying Sklearn Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4405e9e-2cba-446f-9385-c9b653e572fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: rf-scikit-2023-09-26-06-14-31-708\n",
      "INFO:sagemaker:Creating endpoint-config with name Sklearn-Inference-endpoint-2023-09-26-0614\n",
      "INFO:sagemaker:Creating endpoint with name Sklearn-Inference-endpoint-2023-09-26-0614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "initial_instance_count=1\n",
    "# initial_instance_count=1 # Example\n",
    "\n",
    "instance_type='ml.m4.xlarge'\n",
    "# instance_type='ml.m4.xlarge' # Example\n",
    "\n",
    "sklearn_predictor =sklearn_estimator.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=sklearn_endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50577593-9f98-438a-b4bd-3d5847063b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbacb67a-a2b3-4a75-87be-155f9652741f",
   "metadata": {},
   "source": [
    "### Deploying Sagemaker Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee46c04e-f1d7-4803-8554-0b9677759275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_url = f's3://{bucket}/{prefix}/abalone-xgb-built-in-algo/output/sagemaker-xgboost-2023-09-26-05-47-34-821/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc06d3c5-3dde-400a-95c5-610ff3ab0859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# Name of the framework or algorithm\n",
    "framework='xgboost'\n",
    "#framework='xgboost' # Example\n",
    "\n",
    "# Version of the framework or algorithm\n",
    "version = '1.7-1'\n",
    "#version = '0.90-1' # Example\n",
    "\n",
    "# Specify an AWS container image. \n",
    "container = image_uris.retrieve(region=region, \n",
    "                                framework=framework, \n",
    "                                version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be2e6f1d-4212-4eb2-9dd0-406bc1f12cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(image_uri=container, \n",
    "              model_data=model_url, \n",
    "              role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90a55b53-b108-4531-80ce-58c05883a61a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName = Xgboost-Inference-endpoint-2023-09-26-0630\n"
     ]
    }
   ],
   "source": [
    "xgboost_endpoint_name = f\"Xgboost-Inference-endpoint-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"EndpointName =\", xgboost_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "616bd8f4-9d2e-45b1-895f-552bf556e40e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-09-26-06-30-51-235\n",
      "INFO:sagemaker:Creating endpoint-config with name Xgboost-Inference-endpoint-2023-09-26-0630\n",
      "INFO:sagemaker:Creating endpoint with name Xgboost-Inference-endpoint-2023-09-26-0630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "initial_instance_count=1\n",
    "# initial_instance_count=1 # Example\n",
    "\n",
    "instance_type='ml.m4.xlarge'\n",
    "# instance_type='ml.m4.xlarge' # Example\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=xgboost_endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b3266-405c-4072-a82b-d8d34fd0d793",
   "metadata": {},
   "source": [
    "## Real time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e8951bf-2796-48ad-8288-7ba83dc5da99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 306 ms, sys: 17.5 ms, total: 323 ms\n",
      "Wall time: 735 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = (\n",
    "    \"test.csv\"  # customize to your test file, will be 'mnist.single.test' if use data above\n",
    ")\n",
    "prediction_result = []\n",
    "with open(file_name, \"r\") as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "for payload_input in payload.split(\"\\n\")[:10]:\n",
    "    runtime_client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=xgboost_endpoint_name, ContentType=\"text/csv\", Body=payload_input\n",
    "    )\n",
    "    result = response[\"Body\"].read().decode(\"ascii\")\n",
    "    prediction_result.append(int(result.split(\"\\n\")[0][0]))\n",
    "#print(\"Predicted Class Probabilities: {}.\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4ea2d40d-6eee-4a59-960d-68c6d8a9e28f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c44ba27e-29ab-40be-8750-625e4196c981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1430</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>1481</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1057</td>\n",
       "      <td>138</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>1459</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>382</td>\n",
       "      <td>152</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions     0     1     2     3     4     5\n",
       "actuals                                        \n",
       "0            1705     0     0     0     0     0\n",
       "1               0  1430   230     0     0     0\n",
       "2               0   189  1481     0     0     1\n",
       "3               0     0     0  1057   138   427\n",
       "4               0     0     0   111  1459   110\n",
       "5               0     0     5   382   152  1123"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_y['y'].values, columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3d11af0b-e29b-4e0e-b041-818a4c66ac76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### END #######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22959e37-e138-4ecf-b560-45f8d2c20a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
