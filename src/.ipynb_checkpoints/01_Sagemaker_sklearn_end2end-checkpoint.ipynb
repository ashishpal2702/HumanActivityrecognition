{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c095987-0137-4314-8327-3be747780a7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Human Activity Recognition - AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894db4cb-5b82-4573-8cfe-7b16358b696b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem Statement\n",
    "Let’s open the notebook “HAR Model training notebook”. The problem statement for this notebook is: Deploying the Human Activity Recognition problem using the Level 1 MLOps architecture, the aim is to enhance the experience of Blackmi's health app by overcoming the problems faced in the level 0 architecture. Utilising the Human Activity Recognition dataset, we will construct a machine-learning model along with the ML pipelines to categorise user activities for the real-time health alerts using AWS sagemaker studio. Here we will also be monitoring the model performance and deploy the model using different deployment techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57df07-1b14-4dc2-9860-27cdf0ef8c93",
   "metadata": {},
   "source": [
    "### Approach \n",
    "In this notebook we will be building the level 1 architecture of MLOps, and our major focus would be on creating ML pipeline, model monitoring and model deployment. The major take away for this lesson is to learn:\n",
    "\n",
    "1. Feature engineering with the amazon sagemaker processing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2f1123b0-e1a4-4d15-b0eb-0ff261b8f05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries \n",
    "# Importing pandas and numpy for data preprocessing. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Boto3 is used for launching the EC2 instances and manipulating s3 buckets.\n",
    "import boto3\n",
    "# Sagemaker is imported for building, training and deploying machine learning models.\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e63645c-aed6-4890-9d73-93d1b5f6e98b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ap-south-1'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising new sagemaker session as \"sess\".\n",
    "sess = sagemaker.Session()\n",
    "# Check for necessary permission needed for training and deploying models. \n",
    "role = sagemaker.get_execution_role()\n",
    "# To understand where this session is configured to operate.\n",
    "region = boto3.Session().region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6895d1d8-9c61-4e97-a154-280490992c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bucket variable is used for storing the location of the bucket\n",
    "bucket = 'sagemaker-studio-009676737623-l4vs7j0o0ib'\n",
    "# Assigning the prefix variable \n",
    "prefix = 'mlops-level1-data'\n",
    "# input_source variable is used for storing the location of the dataset\n",
    "input_source = 's3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/train_data.gzip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe076e3-81d5-4e62-8e93-f7c48e2eb401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading the dataset using the read_parquet format. \n",
    "df = pd.read_parquet(input_source)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20a303-bb9e-4251-b808-aadb550a03ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bf014-b955-404d-8998-23fb9c71d8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualising the data distribution of the Activity column\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "df.groupby(\"Activity\")[['Activity']].count().plot(kind=\"bar\", title=\"Breakdown by Star Rating\")\n",
    "plt.xlabel(\"Activity\")\n",
    "plt.ylabel(\"Activity Count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eab099-4c31-462b-aeaf-46d5bcc9c1d6",
   "metadata": {},
   "source": [
    "## Feature Engineering with Amazon SageMaker Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741aa1c-4236-4157-a207-d1b02e80ba17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "def _parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Data, model, and output directories\n",
    "    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n",
    "    parser.add_argument('--filepath', type=str, default='/opt/ml/processing/input/')\n",
    "    parser.add_argument('--filename', type=str, default='train_data.gzip')\n",
    "    parser.add_argument('--outputpath', type=str, default='/opt/ml/processing/output/')\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_top_k_features(X, Y, k):\n",
    "        clf = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = clf.fit(X, Y)\n",
    "        feature_df = pd.DataFrame(\n",
    "            data=(X.columns, clf.feature_importances_)\n",
    "        ).T.sort_values(by=1, ascending=False)\n",
    "        cols = feature_df.head(k)[0].values\n",
    "        return cols\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "    # Load data\n",
    "    path = os.path.join(args.filepath,args.filename)\n",
    "    print(path)\n",
    "    \n",
    "    # Reading the dataset and performing label encoding \n",
    "    df = pd.read_parquet(os.path.join(args.filepath,args.filename))\n",
    "    le = LabelEncoder()\n",
    "    df['Activity'] = le.fit_transform(df['Activity'])\n",
    "    df.drop(['date_time'],axis =1 ,inplace = True)\n",
    "\n",
    "    # Assignining the indepeneded and depended variable \n",
    "    X = df.drop(['Activity'], axis =1)\n",
    "    Y = df['Activity']\n",
    "    \n",
    "    # Extracting top 12 important feature and filtering the dataset\n",
    "    k =12\n",
    "    final_cols = get_top_k_features(X, Y, k)\n",
    "    final_cols = np.append(final_cols,np.array(['Activity']))\n",
    "    df = df[final_cols]\n",
    "    \n",
    "    # Train, test, validation split\n",
    "    # Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "    train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=42), [int(0.8 * len(df)), int(0.9 * len(df))])  \n",
    "    \n",
    "    # Storing of train, validation and test datasets \n",
    "    pd.concat([train_data['Activity'], train_data.drop(['Activity'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'train/train.csv'), index=False, header=False)\n",
    "    pd.concat([validation_data['Activity'], validation_data.drop(['Activity'], axis=1)], axis=1).to_csv(os.path.join(args.outputpath, 'validation/validation.csv'), index=False, header=False)\n",
    "    test_data[['Activity']].to_csv(os.path.join(args.outputpath, 'test/test_y.csv'), index=False, header=False)\n",
    "    test_data.drop(['Activity'], axis=1).to_csv(os.path.join(args.outputpath, 'test/test_x.csv'), index=False, header=False)\n",
    "    \n",
    "    ## Save Features columns\n",
    "    dump(final_cols, os.path.join(args.outputpath, 'feature/feature.joblib'))\n",
    "    ## Save Encoder\n",
    "    dump(le, os.path.join(args.outputpath, 'feature/encoder.joblib'))\n",
    "    \n",
    "    print(\"## Processing complete. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522538f-57f2-457a-8936-b11ca432384c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test\"\n",
    "feature_path = f\"s3://{bucket}/{prefix}/feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa725f3b-c253-4c84-afd5-7a24e3a7fa81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary library for data processing \n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=get_execution_role(),\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1, \n",
    "    base_job_name='sklearn-ml-train'\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=input_source, \n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\", \n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=train_path,\n",
    "        ),\n",
    "        ProcessingOutput(output_name=\"validation_data\", source=\"/opt/ml/processing/output/validation\", destination=validation_path),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/output/test\", destination=test_path),\n",
    "        ProcessingOutput(output_name=\"feature_data\", source=\"/opt/ml/processing/output/feature\", destination=feature_path)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03df72-150c-403f-a8ec-045800786489",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c264cd4-10d5-4847-b508-296ea7ee98d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_path.format(bucket, prefix), \n",
    "                                                    content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=validation_path.format(bucket, prefix),\n",
    "                                                     content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1449514a-1c38-489c-a363-e205d4adc54d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sklearn-train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sklearn-train.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load\n",
    "import pandas as pd, numpy as np, os, argparse\n",
    "\n",
    "# inference function - tells SageMaker how to load the model\n",
    "def model_fn(model_dir):\n",
    "    clf = load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "# Argument parser\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=3)\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test.csv\")\n",
    "    # Parse the arguments\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "# Main Training Loop\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "    # Load the dataset\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "    # Separate X and y\n",
    "    X_train, y_train = train_df.drop(train_df.columns[0], axis=1), train_df[train_df.columns[0]]\n",
    "    X_test, y_test = test_df.drop(test_df.columns[0], axis=1), test_df[test_df.columns[0]]\n",
    "    # Define the model and train it\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=args.n_estimators, n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    # Evaluate the model performances\n",
    "    print(f'Model Accuracy: {accuracy_score(y_test, model.predict(X_test))}')\n",
    "    dump(model, os.path.join(args.model_dir, 'model.joblib'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5b4fd364-fe2d-4a93-9305-3035093bcfee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker:Creating training-job with name: rf-scikit-2023-09-18-08-16-32-973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-09-18 08:16:33 Starting - Starting the training job...\n",
      "2023-09-18 08:16:47 Starting - Preparing the instances for training...\n",
      "2023-09-18 08:17:29 Downloading - Downloading input data........\u001b[34m2023-09-18 08:18:41,086 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2023-09-18 08:18:41,088 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 08:18:41,127 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-18 08:18:41,273 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 08:18:41,284 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 08:18:41,294 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-09-18 08:18:41,301 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"min-samples-leaf\": 3,\n",
      "        \"n-estimators\": 120,\n",
      "        \"test-file\": \"validation.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"rf-scikit-2023-09-18-08-16-32-973\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-009676737623/rf-scikit-2023-09-18-08-16-32-973/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn-train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"min-samples-leaf\":3,\"n-estimators\":120,\"test-file\":\"validation.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-009676737623/rf-scikit-2023-09-18-08-16-32-973/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"min-samples-leaf\":3,\"n-estimators\":120,\"test-file\":\"validation.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"rf-scikit-2023-09-18-08-16-32-973\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-009676737623/rf-scikit-2023-09-18-08-16-32-973/source/sourcedir.tar.gz\",\"module_name\":\"sklearn-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--min-samples-leaf\",\"3\",\"--n-estimators\",\"120\",\"--test-file\",\"validation.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MIN-SAMPLES-LEAF=3\u001b[0m\n",
      "\u001b[34mSM_HP_N-ESTIMATORS=120\u001b[0m\n",
      "\u001b[34mSM_HP_TEST-FILE=validation.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python sklearn-train.py --min-samples-leaf 3 --n-estimators 120 --test-file validation.csv\u001b[0m\n",
      "\n",
      "2023-09-18 08:19:00 Training - Training image download completed. Training in progress.\n",
      "2023-09-18 08:19:00 Uploading - Uploading generated training model\u001b[34mModel Accuracy: 0.7106710671067107\u001b[0m\n",
      "\u001b[34m2023-09-18 08:18:54,859 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-18 08:19:16 Completed - Training job completed\n",
      "Training seconds: 107\n",
      "Billable seconds: 107\n"
     ]
    }
   ],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "# Define the Estimator from SageMaker (Script Mode)\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"sklearn-train.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"rf-scikit\",\n",
    "    metric_definitions=[{\"Name\": \"model_accuracy\", \"Regex\": \"Model Accuracy: ([0-9.]+).*$\"}],\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": 120,\n",
    "        \"min-samples-leaf\": 3,\n",
    "        \"test-file\": \"validation.csv\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Train the model (~5 minutes)\n",
    "sklearn_estimator.fit({\"train\": s3_input_train, \"test\": s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67abcfa8-6342-45bd-b81e-d664df4f5fd3",
   "metadata": {},
   "source": [
    "## Hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0f748c15-156b-43d7-a970-36a33b7ebc61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: rf-scikit-2023-09-18-08-21-35-397\n",
      "INFO:sagemaker:Creating endpoint-config with name rf-scikit-2023-09-18-08-21-35-397\n",
      "INFO:sagemaker:Creating endpoint with name rf-scikit-2023-09-18-08-21-35-397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# Deploying the model\n",
    "sklearn_predictor = sklearn_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b3266-405c-4072-a82b-d8d34fd0d793",
   "metadata": {},
   "source": [
    "## Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "221a2680-a4ad-4da2-843d-38a178584554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sklearn_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d6ff9d25-00a2-4663-bbd4-65f5803a25de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/test/test_x.csv to tmp/test_x.csv\n",
      "download: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/test/test_y.csv to tmp/test_y.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $test_path/test_x.csv ./tmp/test_x.csv\n",
    "!aws s3 cp $test_path/test_y.csv ./tmp/test_y.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7fd56d99-525f-43d8-ad14-2cfbb02131cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_x = pd.read_csv('tmp/test_x.csv', names=[f'{i}' for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2c09cfb-b355-4f07-a1e5-e15c9b11a2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_x = pd.read_csv('tmp/test_x.csv', names=[f'{i}' for i in range(10)])\n",
    "test_y = pd.read_csv('tmp/test_y.csv', names=['y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6271cabb-1458-42ea-8aa7-0f1526b3e5e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = sklearn_predictor.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ec98ce6b-ec1a-49c4-a892-a049b6ae626a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1705\n",
      "           1       0.81      0.82      0.82      1660\n",
      "           2       0.75      0.56      0.64      1671\n",
      "           3       0.43      0.53      0.47      1622\n",
      "           4       0.81      0.72      0.76      1680\n",
      "           5       0.56      0.63      0.59      1662\n",
      "\n",
      "    accuracy                           0.71     10000\n",
      "   macro avg       0.73      0.71      0.71     10000\n",
      "weighted avg       0.73      0.71      0.72     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y['y'].values,  predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c44ba27e-29ab-40be-8750-625e4196c981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actuals</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1368</td>\n",
       "      <td>163</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>941</td>\n",
       "      <td>372</td>\n",
       "      <td>21</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>121</td>\n",
       "      <td>853</td>\n",
       "      <td>119</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>1217</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>34</td>\n",
       "      <td>392</td>\n",
       "      <td>137</td>\n",
       "      <td>1039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions     0     1    2    3     4     5\n",
       "actuals                                      \n",
       "0            1705     0    0    0     0     0\n",
       "1               0  1368  163   85     9    35\n",
       "2               0   180  941  372    21   157\n",
       "3               0    73  121  853   119   456\n",
       "4               0     6    3  279  1217   175\n",
       "5               0    60   34  392   137  1039"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_y['y'].values, columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1a865-7a9a-4df5-ab98-56e3ef97f492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f808c-4f98-423b-8ad8-c3bd5879dcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef473a9-c577-4b81-ae6f-c73ffd5623ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d5b36-1ce3-4c02-83fd-705c3cd89724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
