{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Model Monitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "* Host a machine learning model in Amazon SageMaker and capture inference requests, results, and metadata \n",
    "* Analyze a training dataset to generate baseline constraints\n",
    "* Monitor a live endpoint for violations against constraints\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that encompasses the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. You can deploy your models to production with Amazon SageMaker to make predictions and lower costs than was previously possible.\n",
    "\n",
    "In addition, Amazon SageMaker enables you to capture the input, output and metadata for invocations of the models that you deploy. It also enables you to analyze the data and monitor its quality. In this notebook, you learn how Amazon SageMaker enables these capabilities.\n",
    "\n",
    "## Runtime\n",
    "\n",
    "This notebook uses an hourly monitor, so it takes between 30-90 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [PART A: Capturing real-time inference data from Amazon SageMaker endpoints](#PART-A:-Capturing-real-time-inference-data-from-Amazon-SageMaker-endpoints)\n",
    "1. [PART B: Model Monitor - Baselining and continuous monitoring](#PART-B:-Model-Monitor---Baselining-and-continuous-monitoring)\n",
    "    1. [Constraint suggestion with baseline/training dataset](#1.-Constraint-suggestion-with-baseline/training-dataset)\n",
    "    1. [Analyze collected data for data quality issues](#2.-Analyze-collected-data-for-data-quality-issues)\n",
    "---\n",
    "## Setup\n",
    "\n",
    "To get started, make sure you have these prerequisites completed:\n",
    "\n",
    "* Specify an AWS Region to host your model.\n",
    "* An IAM role ARN exists that is used to give Amazon SageMaker access to your data in Amazon Simple Storage Service (Amazon S3).\n",
    "* Use the default S3 bucket to store the data used to train your model, any additional model data, and the data captured from model invocations. For demonstration purposes, you are using the same bucket for these. In reality, you might want to separate them with different security policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role ARN: arn:aws:iam::009676737623:role/sagemaker-full-permission\n",
      "Demo Bucket: sagemaker-studio-009676737623-l4vs7j0o0ib\n",
      "Capture path: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/datacapture\n",
      "Report path: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/reports\n",
      "Preproc Code path: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/code/preprocessor.py\n",
      "Postproc Code path: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/code/postprocessor.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, session\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "region = sm_session.boto_region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"Role ARN: {}\".format(role))\n",
    "\n",
    "bucket = 'sagemaker-studio-009676737623-l4vs7j0o0ib' #sm_session.default_bucket()\n",
    "print(\"Demo Bucket: {}\".format(bucket))\n",
    "prefix = 'mlops-level1-data'\n",
    "\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "code_prefix = \"{}/code\".format(prefix)\n",
    "s3_code_preprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"preprocessor.py\")\n",
    "s3_code_postprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"postprocessor.py\")\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: Capturing real-time inference data from Amazon SageMaker endpoints\n",
    "Create an endpoint to showcase the data capture capability in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the deployed model\n",
    "\n",
    "You can now send data to this endpoint to get inferences in real time. Because you enabled the data capture in the previous steps, the request and response payload, along with some additional metadata, is saved in the Amazon Simple Storage Service (Amazon S3) location you have specified in the DataCaptureConfig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for about 3 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = 'xgboost-inference-2023-09-25-0911'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "mlops-level1-data/datacapture/xgboost-inference-2023-09-25-0911/AllTraffic/2023/09/25/09/18-21-625-e029529b-f453-403c-83ea-942ffc4f695d.jsonl\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the contents of a single capture file. Here you should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"-0.84124676,0.96339614,0.89946864,0.89205451,0.97743631,-0.16126549,-0.93472378,-0.14083968,-0.12321341,0.17994061,-1.0,-0.97090521\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1.0\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"0854c4a7-ce4c-4629-952f-a8fc4f96c413\",\"inferenceTime\":\"2023-09-25T09:18:21Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"-0.8447876,0.96656113,0.9078289,0.89206031,0.98452014,-0.16134256,-0.94306751,-0.14155127,-0.11489334,0.18028889,-1.0,-0.97058275\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1.0\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"4a4606cf-dcf4-414c-8eba-1b81ca718de6\",\"inferenceTime\":\"2023-09-25T09:18:21Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"-0.84893347,0.9668781,0.90866783,0.89240056,0.9867701,-0.16371122,-0.93869155,-0.14200984,-0.11489334,0.18063731,-1.0,-0.97036812\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1.0\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"a57c0317-372c-4e99-91aa-debb5a6d1f5f\",\"inferenceTime\":\"2023-09-25T09:18:21Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"-0.84864938,0.96761519,0.91062071,0.89381715,0.98682105,-0.16371122,-0.93869155,-0.14397645,-0.12133575,0.18193476,-1.0,-0.96939971\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"1.0\\n\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"f3d359d9-cf32-4b6b-8958-3de6304ba593\",\"inferenceTime\":\"2023-09-25T09:18:21Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"IN\n"
     ]
    }
   ],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file so that you can observe a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"-0.84124676,0.96339614,0.89946864,0.89205451,0.97743631,-0.16126549,-0.93472378,-0.14083968,-0.12321341,0.17994061,-1.0,-0.97090521\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"1.0\\n\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"0854c4a7-ce4c-4629-952f-a8fc4f96c413\",\n",
      "    \"inferenceTime\": \"2023-09-25T09:18:21Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In the example, you provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, you expose the encoding that you used to encode the input and output payloads in the capture format with the `encoding` value.\n",
    "\n",
    "To recap, you observed how you can enable capturing the input or output payloads to an endpoint with a new parameter. You have also observed what the captured format looks like in Amazon S3. Next, continue to explore how Amazon SageMaker helps with monitoring the data collected in Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B: Model Monitor - Baselining and continuous monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, Amazon SageMaker provides the capability for you to monitor and evaluate the data observed by the endpoints. For this:\n",
    "1. Create a baseline with which you compare the realtime traffic. \n",
    "1. Once a baseline is ready, setup a schedule to continously evaluate and compare against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Constraint suggestion with baseline/training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the training dataset you can ask Amazon SageMaker to suggest a set of baseline `constraints` and generate descriptive `statistics` to explore the data. For this example, upload the training dataset that was used to train the pre-trained model included in this example. If you already have it in Amazon S3, you can directly point to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/baseline/data\n",
      "Baseline results uri: s3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/baseline/results\n"
     ]
    }
   ],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + \"/baseline\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a baselining job with training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the training data ready in Amazon S3, start a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` starts a `ProcessingJob` using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-09-25-09-22-39-226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................\u001b[34m2023-09-25 09:26:43.891041: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:43.891080: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:45.419737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:45.419764: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:45.419785: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-149-1.ap-south-1.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:45.420043: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:46,969 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:ap-south-1:009676737623:processing-job/baseline-suggestion-job-2023-09-25-09-22-39-226', 'ProcessingJobName': 'baseline-suggestion-job-2023-09-25-09-22-39-226', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '126357580389.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/baseline/data/training-inputs-with-header.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-studio-009676737623-l4vs7j0o0ib/mlops-level1-data/baseline/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::009676737623:role/sagemaker-full-permission', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:46,969 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:46,969 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:46,970 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:46,970 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:46,970 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,027 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,027 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,027 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,036 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,037 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,037 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,557 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.149.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/sh\u001b[0m\n",
      "\u001b[34mare/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_382\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,565 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:47,568 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-ca4ab369-93f4-4819-ba57-8f9ee0882f8a\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,085 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,096 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,097 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,099 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,104 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,104 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,104 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,104 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,135 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,146 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,146 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,150 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,153 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Sep 25 09:26:48\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,155 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,155 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,156 INFO util.GSet: 2.0% max memory 3.1 GB = 63.7 MB\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,156 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,253 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,257 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,283 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,283 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,283 INFO util.GSet: 1.0% max memory 3.1 GB = 31.8 MB\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,283 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,285 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,285 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,285 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,285 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,290 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,293 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,293 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,293 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,293 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,299 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,299 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,299 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,302 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,303 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,304 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,304 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,304 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 978.1 KB\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,304 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,325 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1187926670-10.0.149.1-1695634008318\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,337 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,344 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,421 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,433 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,436 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.149.1\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:48,446 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:50,506 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:50,507 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:52,579 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:52,579 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:54,755 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:54,755 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:56,875 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:56,875 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:59,046 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:26:59,047 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:09,055 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:10,576 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:10,953 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:10,987 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:10,997 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,570 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,599 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,600 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,600 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,601 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,630 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,647 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,649 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,711 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,712 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,712 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,713 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:11,713 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,130 INFO util.Utils: Successfully started service 'sparkDriver' on port 37363.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,156 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,190 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,210 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,210 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,249 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,281 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-1478b7bf-de7a-4be5-bf76-f52174d22e74\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,301 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,350 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:12,392 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.149.1:37363/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1695634031566\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,011 INFO client.RMProxy: Connecting to ResourceManager at /10.0.149.1:8032\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,696 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,696 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,702 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,703 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,703 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,703 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,709 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:13,791 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:15,665 INFO yarn.Client: Uploading resource file:/tmp/spark-c767a7cc-9658-4174-8656-55b97a79ae37/__spark_libs__790904502557699739.zip -> hdfs://10.0.149.1/user/root/.sparkStaging/application_1695634014337_0001/__spark_libs__790904502557699739.zip\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,430 INFO yarn.Client: Uploading resource file:/tmp/spark-c767a7cc-9658-4174-8656-55b97a79ae37/__spark_conf__7941992442577247215.zip -> hdfs://10.0.149.1/user/root/.sparkStaging/application_1695634014337_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,484 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,484 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,485 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,485 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,485 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,512 INFO yarn.Client: Submitting application application_1695634014337_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:17,724 INFO impl.YarnClientImpl: Submitted application application_1695634014337_0001\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:18,728 INFO yarn.Client: Application report for application_1695634014337_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:18,732 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1695634037611\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1695634014337_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:19,734 INFO yarn.Client: Application report for application_1695634014337_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:20,739 INFO yarn.Client: Application report for application_1695634014337_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:21,743 INFO yarn.Client: Application report for application_1695634014337_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,393 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1695634014337_0001), /proxy/application_1695634014337_0001\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,749 INFO yarn.Client: Application report for application_1695634014337_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,750 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.149.1\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1695634037611\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1695634014337_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,753 INFO cluster.YarnClientSchedulerBackend: Application application_1695634014337_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,789 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44113.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,789 INFO netty.NettyBlockTransferService: Server created on 10.0.149.1:44113\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,791 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,800 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.149.1, 44113, None)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,804 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.149.1:44113 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.149.1, 44113, None)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,810 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.149.1, 44113, None)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,812 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.149.1, 44113, None)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:22,951 INFO util.log: Logging initialized @13722ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:23,821 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:28,013 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.149.1:34008) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:28,201 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:46351 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 46351, None)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:42,881 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:42,985 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:43,027 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:43,030 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:43,943 INFO datasources.InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,094 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,341 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,344 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.149.1:44113 (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,347 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,651 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,653 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,656 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 2441537\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,702 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,718 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,718 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,719 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,720 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,725 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,753 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,756 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,756 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.149.1:44113 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,758 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,774 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,775 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:44,811 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4640 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:45,109 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:46351 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,101 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:46351 (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,478 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1679 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,480 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,486 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.739 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,490 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,491 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,493 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.790391 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,665 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.149.1:44113 in memory (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,674 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:46351 in memory (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,688 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.149.1:44113 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:46,695 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:46351 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,657 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,659 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,661 INFO datasources.FileSourceStrategy: Output Data Schema: struct<tGravityAcc-energy()-X: string, tGravityAcc-min()-X: string, tGravityAcc-max()-X: string, tGravityAcc-max()-Y: string, tGravityAcc-min()-Y: string ... 11 more fields>\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,877 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,894 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,894 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.149.1:44113 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,895 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,908 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,952 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,953 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,954 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,954 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,956 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:48,958 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,028 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.6 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,030 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,031 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.149.1:44113 (size: 8.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,032 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,033 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,033 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,037 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,096 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:46351 (size: 8.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:49,944 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:46351 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,307 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:46351 (size: 2.6 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,464 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1430 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,464 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,465 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.503 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,466 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,466 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,467 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.514484 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:50,687 INFO codegen.CodeGenerator: Code generated in 163.518763 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,174 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,310 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,313 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,313 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,314 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,315 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,317 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,338 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 114.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,340 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,340 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.149.1:44113 (size: 34.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,341 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,343 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,343 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,351 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:51,371 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:46351 (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,670 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1321 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,670 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,672 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.352 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,673 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,673 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,673 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,674 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,748 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,750 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,750 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,750 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,750 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,751 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,762 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 167.8 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,764 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,765 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.149.1:44113 (size: 46.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,765 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,766 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,766 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,770 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,788 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:46351 (size: 46.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:52,950 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,316 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 548 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,316 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,318 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.562 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,319 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,319 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,320 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.571826 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,357 INFO codegen.CodeGenerator: Code generated in 26.523883 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,634 INFO codegen.CodeGenerator: Code generated in 25.743974 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,703 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,705 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,705 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,705 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,706 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,709 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,729 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,731 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,731 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.149.1:44113 (size: 16.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,733 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,734 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,734 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,735 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:53,753 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:46351 (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,299 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 564 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,299 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,300 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.590 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,300 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,300 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,301 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.597274 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,762 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:46351 in memory (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,764 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.149.1:44113 in memory (size: 34.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,815 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.149.1:44113 in memory (size: 46.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,816 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:46351 in memory (size: 46.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,864 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.149.1:44113 in memory (size: 16.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,869 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:46351 in memory (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,907 INFO codegen.CodeGenerator: Code generated in 147.505962 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,915 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,915 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,915 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,915 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,916 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,917 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,917 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:46351 in memory (size: 8.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,922 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.149.1:44113 in memory (size: 8.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,936 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.8 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,938 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,938 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.149.1:44113 (size: 23.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,939 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,939 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,939 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,941 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:54,960 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:46351 (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,186 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 245 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,187 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,188 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.266 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,188 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,188 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,188 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,188 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,404 INFO codegen.CodeGenerator: Code generated in 127.806243 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,416 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,418 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,418 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,418 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,418 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,419 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,422 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,425 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,425 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.149.1:44113 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,426 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,427 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,427 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,428 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,443 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:46351 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,451 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,519 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 91 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,519 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,521 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.101 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,522 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,522 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,522 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.105576 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:55,616 INFO codegen.CodeGenerator: Code generated in 72.265682 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,012 INFO scheduler.DAGScheduler: Registering RDD 42 (collect at AnalysisRunner.scala:326) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,012 INFO scheduler.DAGScheduler: Got map stage job 7 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,013 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,013 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,014 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,016 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[42] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,022 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 83.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,024 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,024 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.149.1:44113 (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,025 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,025 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[42] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,025 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,027 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,045 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:46351 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,317 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 291 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,318 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,320 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at AnalysisRunner.scala:326) finished in 0.303 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,321 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,321 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,321 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,321 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,367 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,369 INFO scheduler.DAGScheduler: Got job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,369 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,373 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,374 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,375 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[45] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,384 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 168.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,387 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 46.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,387 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.149.1:44113 (size: 46.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,388 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,388 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[45] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,388 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,389 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,400 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:46351 (size: 46.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,412 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,525 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 136 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,525 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,528 INFO scheduler.DAGScheduler: ResultStage 11 (collect at AnalysisRunner.scala:326) finished in 0.151 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,528 INFO scheduler.DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,528 INFO cluster.YarnScheduler: Killing all running tasks in stage 11: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,528 INFO scheduler.DAGScheduler: Job 8 finished: collect at AnalysisRunner.scala:326, took 0.160664 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,676 INFO codegen.CodeGenerator: Code generated in 19.510564 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,710 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,712 INFO scheduler.DAGScheduler: Got job 9 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,713 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,713 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,714 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,715 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[55] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,727 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 38.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,729 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,729 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.149.1:44113 (size: 16.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,730 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,730 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[55] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,730 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,732 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:56,753 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:46351 (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,073 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 341 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,074 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,075 INFO scheduler.DAGScheduler: ResultStage 12 (treeReduce at KLLRunner.scala:107) finished in 0.359 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,076 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,076 INFO cluster.YarnScheduler: Killing all running tasks in stage 12: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,077 INFO scheduler.DAGScheduler: Job 9 finished: treeReduce at KLLRunner.scala:107, took 0.366172 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,569 INFO codegen.CodeGenerator: Code generated in 203.415893 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,601 INFO scheduler.DAGScheduler: Registering RDD 60 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,602 INFO scheduler.DAGScheduler: Got map stage job 10 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,602 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,602 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,604 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,611 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[60] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,632 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 73.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,635 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,636 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.149.1:44113 (size: 23.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,637 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,639 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[60] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,639 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,643 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,672 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:46351 (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,802 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 160 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,802 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,804 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (collect at AnalysisRunner.scala:326) finished in 0.189 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,811 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,811 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,812 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:57,812 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,152 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.149.1:44113 in memory (size: 23.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,161 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:46351 in memory (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,209 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,212 INFO scheduler.DAGScheduler: Got job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,212 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,213 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,213 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,221 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[63] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,226 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.149.1:44113 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,227 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:46351 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,236 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 66.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,239 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,240 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.149.1:44113 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,241 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,242 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[63] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,246 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,248 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 11) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,281 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:46351 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,290 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,291 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:46351 in memory (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,305 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.149.1:44113 in memory (size: 16.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,309 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 11) in 61 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,310 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,311 INFO scheduler.DAGScheduler: ResultStage 15 (collect at AnalysisRunner.scala:326) finished in 0.089 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,311 INFO scheduler.DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,311 INFO cluster.YarnScheduler: Killing all running tasks in stage 15: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,311 INFO scheduler.DAGScheduler: Job 11 finished: collect at AnalysisRunner.scala:326, took 0.101730 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,422 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:46351 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,440 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.149.1:44113 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,543 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:46351 in memory (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,556 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.149.1:44113 in memory (size: 23.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,633 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.149.1:44113 in memory (size: 46.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,636 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:46351 in memory (size: 46.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,926 INFO scheduler.DAGScheduler: Registering RDD 68 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,927 INFO scheduler.DAGScheduler: Got map stage job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,927 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,927 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,928 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,929 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,938 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 63.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,940 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,941 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.149.1:44113 (size: 22.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,942 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,943 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,943 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,945 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:58,963 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:46351 (size: 22.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,308 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 12) in 362 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,308 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,309 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326) finished in 0.377 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,309 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,311 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,311 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,312 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,372 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,375 INFO scheduler.DAGScheduler: Got job 13 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,375 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,375 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,376 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,376 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,388 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 115.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,390 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,390 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.149.1:44113 (size: 35.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,392 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,392 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,393 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,395 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,408 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:46351 (size: 35.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,426 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,591 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 13) in 197 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,591 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,592 INFO scheduler.DAGScheduler: ResultStage 18 (collect at AnalysisRunner.scala:326) finished in 0.212 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,594 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,595 INFO cluster.YarnScheduler: Killing all running tasks in stage 18: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,596 INFO scheduler.DAGScheduler: Job 13 finished: collect at AnalysisRunner.scala:326, took 0.222933 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,640 INFO codegen.CodeGenerator: Code generated in 36.957825 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,791 INFO codegen.CodeGenerator: Code generated in 24.17192 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,833 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,836 INFO scheduler.DAGScheduler: Got job 14 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,836 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,837 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,842 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,844 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[81] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,862 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 36.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,865 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,865 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.149.1:44113 (size: 16.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,866 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,867 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[81] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,867 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,869 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4968 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:27:59,891 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:46351 (size: 16.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,548 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 14) in 1679 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,548 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,549 INFO scheduler.DAGScheduler: ResultStage 19 (treeReduce at KLLRunner.scala:107) finished in 1.703 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,550 INFO scheduler.DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,550 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,551 INFO scheduler.DAGScheduler: Job 14 finished: treeReduce at KLLRunner.scala:107, took 1.717257 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,736 INFO codegen.CodeGenerator: Code generated in 46.564625 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,745 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,745 INFO scheduler.DAGScheduler: Got map stage job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,745 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,745 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,746 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,747 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,751 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 42.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,752 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,753 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.149.1:44113 (size: 16.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,754 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,755 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,756 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,757 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 15) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,770 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:46351 (size: 16.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,896 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 15) in 139 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,896 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,898 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.149 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,898 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,901 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,901 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:01,901 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,067 INFO codegen.CodeGenerator: Code generated in 83.092736 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,090 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,092 INFO scheduler.DAGScheduler: Got job 16 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,092 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,093 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,093 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,094 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,097 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 32.6 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,106 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,107 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.149.1:44113 (size: 11.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,108 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,109 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,109 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,111 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 16) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,124 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:46351 (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,130 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,177 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 16) in 67 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,177 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,179 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,179 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,179 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,180 INFO scheduler.DAGScheduler: Job 16 finished: collect at AnalysisRunner.scala:326, took 0.089272 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,234 INFO codegen.CodeGenerator: Code generated in 41.080627 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,321 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,326 INFO scheduler.DAGScheduler: Registering RDD 97 (countByKey at ColumnProfiler.scala:592) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,327 INFO scheduler.DAGScheduler: Got job 17 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,327 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,327 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,327 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,330 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[97] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,337 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 30.6 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,339 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,340 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.149.1:44113 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,341 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,341 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[97] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,342 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,343 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 17) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,356 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:46351 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,595 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 17) in 252 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,595 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,597 INFO scheduler.DAGScheduler: ShuffleMapStage 23 (countByKey at ColumnProfiler.scala:592) finished in 0.266 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,598 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,598 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,598 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 24)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,599 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,599 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (ShuffledRDD[98] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,602 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,605 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,611 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.149.1:44113 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,612 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,612 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[98] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,613 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,615 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,627 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:46351 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,634 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,671 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 57 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,671 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,672 INFO scheduler.DAGScheduler: ResultStage 24 (countByKey at ColumnProfiler.scala:592) finished in 0.072 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,673 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,674 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,674 INFO scheduler.DAGScheduler: Job 17 finished: countByKey at ColumnProfiler.scala:592, took 0.353630 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,936 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,977 INFO codegen.CodeGenerator: Code generated in 10.837367 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,984 INFO scheduler.DAGScheduler: Registering RDD 103 (count at StatsGenerator.scala:66) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,984 INFO scheduler.DAGScheduler: Got map stage job 18 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,984 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,984 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,985 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,985 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,990 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 22.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,992 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,992 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.149.1:44113 (size: 10.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,992 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,994 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,994 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:02,996 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4957 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,010 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:46351 (size: 10.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,176 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 180 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,176 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,180 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (count at StatsGenerator.scala:66) finished in 0.194 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,180 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,180 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,180 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,180 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,212 INFO codegen.CodeGenerator: Code generated in 11.218523 ms\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,222 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,223 INFO scheduler.DAGScheduler: Got job 19 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,223 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,223 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,223 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,224 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[106] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,226 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 11.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,227 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,228 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.149.1:44113 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,228 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,228 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[106] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,229 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,230 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,240 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:46351 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,244 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.149.1:34008\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,262 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 20) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,263 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,265 INFO scheduler.DAGScheduler: ResultStage 27 (count at StatsGenerator.scala:66) finished in 0.041 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,265 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,265 INFO cluster.YarnScheduler: Killing all running tasks in stage 27: Stage finished\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,266 INFO scheduler.DAGScheduler: Job 19 finished: count at StatsGenerator.scala:66, took 0.043628 s\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,699 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.149.1:44113 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,703 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:46351 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,719 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.149.1:44113 in memory (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,720 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:46351 in memory (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,740 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:46351 in memory (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,752 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.149.1:44113 in memory (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,775 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.149.1:44113 in memory (size: 22.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,781 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:46351 in memory (size: 22.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,787 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,814 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,864 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,865 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,875 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,897 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,954 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,959 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,973 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:03,980 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,036 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,037 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,037 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,078 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,079 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c767a7cc-9658-4174-8656-55b97a79ae37\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,104 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ead0e6fe-97a1-4d8d-b274-709e460e557f\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,178 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-09-25 09:28:04,178 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f39c0b09420>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-inputs-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "mlops-level1-data/baseline/results/constraints.json\n",
      " mlops-level1-data/baseline/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "      <th>string_statistics.common.num_present</th>\n",
       "      <th>string_statistics.common.num_missing</th>\n",
       "      <th>string_statistics.distinct_count</th>\n",
       "      <th>string_statistics.distribution.categorical.buckets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tGravityAcc-energy()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244478</td>\n",
       "      <td>2444.776117</td>\n",
       "      <td>0.405491</td>\n",
       "      <td>-0.998007</td>\n",
       "      <td>0.925203</td>\n",
       "      <td>[{'lower_bound': -0.998007486265012, 'upper_bo...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0559418828254526, 0.8839898088232856, 0.39...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tGravityAcc-min()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.358796</td>\n",
       "      <td>3587.958730</td>\n",
       "      <td>0.345600</td>\n",
       "      <td>-0.834682</td>\n",
       "      <td>0.990244</td>\n",
       "      <td>[{'lower_bound': -0.8346816131300285, 'upper_b...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.4776788532259567, 0.1438595980947464, 0.36...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tGravityAcc-max()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321641</td>\n",
       "      <td>3216.410348</td>\n",
       "      <td>0.333667</td>\n",
       "      <td>-0.930590</td>\n",
       "      <td>0.935814</td>\n",
       "      <td>[{'lower_bound': -0.930589583620698, 'upper_bo...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0543181167012687, 0.5515309041056298, 0.69...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tGravityAcc-max()-Y</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.016499</td>\n",
       "      <td>-164.991533</td>\n",
       "      <td>0.199114</td>\n",
       "      <td>-0.881894</td>\n",
       "      <td>0.910844</td>\n",
       "      <td>[{'lower_bound': -0.8818940295426656, 'upper_b...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.0203217192045044, -0.1011029908678471, -0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tGravityAcc-min()-Y</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002366</td>\n",
       "      <td>-23.657560</td>\n",
       "      <td>0.209443</td>\n",
       "      <td>-0.820008</td>\n",
       "      <td>0.960677</td>\n",
       "      <td>[{'lower_bound': -0.8200084492391537, 'upper_b...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.0561420526400298, -0.0860616255750889, -0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>angle(X,gravityMean)</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.259722</td>\n",
       "      <td>-2597.224467</td>\n",
       "      <td>0.318041</td>\n",
       "      <td>-0.962746</td>\n",
       "      <td>0.948844</td>\n",
       "      <td>[{'lower_bound': -0.9627455239464414, 'upper_b...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.0750145467099326, -0.3632806024648241, -0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tGravityAcc-mean()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.350513</td>\n",
       "      <td>3505.125622</td>\n",
       "      <td>0.345758</td>\n",
       "      <td>-0.928908</td>\n",
       "      <td>0.968281</td>\n",
       "      <td>[{'lower_bound': -0.928908482428232, 'upper_bo...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.6127667738354671, 0.3660023320497671, 0.33...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tGravityAcc-mean()-Y</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.011196</td>\n",
       "      <td>-111.960140</td>\n",
       "      <td>0.207904</td>\n",
       "      <td>-0.987302</td>\n",
       "      <td>0.970777</td>\n",
       "      <td>[{'lower_bound': -0.9873023558651276, 'upper_b...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.0974610469800742, -0.0533163186823823, -0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tBodyAccMag-max()</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.253560</td>\n",
       "      <td>-2535.602898</td>\n",
       "      <td>0.306597</td>\n",
       "      <td>-0.994979</td>\n",
       "      <td>0.783483</td>\n",
       "      <td>[{'lower_bound': -0.9949789816739202, 'upper_b...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.3789222192625217, 0.024732844333505, -0.3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tBodyGyroJerk-entropy()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015844</td>\n",
       "      <td>158.438494</td>\n",
       "      <td>0.327757</td>\n",
       "      <td>-0.900523</td>\n",
       "      <td>0.830702</td>\n",
       "      <td>[{'lower_bound': -0.9005232054558837, 'upper_b...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.0118381331548275, 0.0874415343464096, -0....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name inferred_type  \\\n",
       "0     tGravityAcc-energy()-X    Fractional   \n",
       "1        tGravityAcc-min()-X    Fractional   \n",
       "2        tGravityAcc-max()-X    Fractional   \n",
       "3        tGravityAcc-max()-Y    Fractional   \n",
       "4        tGravityAcc-min()-Y    Fractional   \n",
       "5       angle(X,gravityMean)    Fractional   \n",
       "6       tGravityAcc-mean()-X    Fractional   \n",
       "7       tGravityAcc-mean()-Y    Fractional   \n",
       "8          tBodyAccMag-max()    Fractional   \n",
       "9  tBodyGyroJerk-entropy()-X    Fractional   \n",
       "\n",
       "   numerical_statistics.common.num_present  \\\n",
       "0                                  10000.0   \n",
       "1                                  10000.0   \n",
       "2                                  10000.0   \n",
       "3                                  10000.0   \n",
       "4                                  10000.0   \n",
       "5                                  10000.0   \n",
       "6                                  10000.0   \n",
       "7                                  10000.0   \n",
       "8                                  10000.0   \n",
       "9                                  10000.0   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                      0.0                   0.244478   \n",
       "1                                      0.0                   0.358796   \n",
       "2                                      0.0                   0.321641   \n",
       "3                                      0.0                  -0.016499   \n",
       "4                                      0.0                  -0.002366   \n",
       "5                                      0.0                  -0.259722   \n",
       "6                                      0.0                   0.350513   \n",
       "7                                      0.0                  -0.011196   \n",
       "8                                      0.0                  -0.253560   \n",
       "9                                      0.0                   0.015844   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0               2444.776117                      0.405491   \n",
       "1               3587.958730                      0.345600   \n",
       "2               3216.410348                      0.333667   \n",
       "3               -164.991533                      0.199114   \n",
       "4                -23.657560                      0.209443   \n",
       "5              -2597.224467                      0.318041   \n",
       "6               3505.125622                      0.345758   \n",
       "7               -111.960140                      0.207904   \n",
       "8              -2535.602898                      0.306597   \n",
       "9                158.438494                      0.327757   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                 -0.998007                  0.925203   \n",
       "1                 -0.834682                  0.990244   \n",
       "2                 -0.930590                  0.935814   \n",
       "3                 -0.881894                  0.910844   \n",
       "4                 -0.820008                  0.960677   \n",
       "5                 -0.962746                  0.948844   \n",
       "6                 -0.928908                  0.968281   \n",
       "7                 -0.987302                  0.970777   \n",
       "8                 -0.994979                  0.783483   \n",
       "9                 -0.900523                  0.830702   \n",
       "\n",
       "       numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': -0.998007486265012, 'upper_bo...   \n",
       "1  [{'lower_bound': -0.8346816131300285, 'upper_b...   \n",
       "2  [{'lower_bound': -0.930589583620698, 'upper_bo...   \n",
       "3  [{'lower_bound': -0.8818940295426656, 'upper_b...   \n",
       "4  [{'lower_bound': -0.8200084492391537, 'upper_b...   \n",
       "5  [{'lower_bound': -0.9627455239464414, 'upper_b...   \n",
       "6  [{'lower_bound': -0.928908482428232, 'upper_bo...   \n",
       "7  [{'lower_bound': -0.9873023558651276, 'upper_b...   \n",
       "8  [{'lower_bound': -0.9949789816739202, 'upper_b...   \n",
       "9  [{'lower_bound': -0.9005232054558837, 'upper_b...   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                               0.64           \n",
       "1                                               0.64           \n",
       "2                                               0.64           \n",
       "3                                               0.64           \n",
       "4                                               0.64           \n",
       "5                                               0.64           \n",
       "6                                               0.64           \n",
       "7                                               0.64           \n",
       "8                                               0.64           \n",
       "9                                               0.64           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                             2048.0           \n",
       "1                                             2048.0           \n",
       "2                                             2048.0           \n",
       "3                                             2048.0           \n",
       "4                                             2048.0           \n",
       "5                                             2048.0           \n",
       "6                                             2048.0           \n",
       "7                                             2048.0           \n",
       "8                                             2048.0           \n",
       "9                                             2048.0           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.data  \\\n",
       "0  [[0.0559418828254526, 0.8839898088232856, 0.39...   \n",
       "1  [[0.4776788532259567, 0.1438595980947464, 0.36...   \n",
       "2  [[0.0543181167012687, 0.5515309041056298, 0.69...   \n",
       "3  [[-0.0203217192045044, -0.1011029908678471, -0...   \n",
       "4  [[-0.0561420526400298, -0.0860616255750889, -0...   \n",
       "5  [[-0.0750145467099326, -0.3632806024648241, -0...   \n",
       "6  [[0.6127667738354671, 0.3660023320497671, 0.33...   \n",
       "7  [[-0.0974610469800742, -0.0533163186823823, -0...   \n",
       "8  [[-0.3789222192625217, 0.024732844333505, -0.3...   \n",
       "9  [[-0.0118381331548275, 0.0874415343464096, -0....   \n",
       "\n",
       "   string_statistics.common.num_present  string_statistics.common.num_missing  \\\n",
       "0                                   NaN                                   NaN   \n",
       "1                                   NaN                                   NaN   \n",
       "2                                   NaN                                   NaN   \n",
       "3                                   NaN                                   NaN   \n",
       "4                                   NaN                                   NaN   \n",
       "5                                   NaN                                   NaN   \n",
       "6                                   NaN                                   NaN   \n",
       "7                                   NaN                                   NaN   \n",
       "8                                   NaN                                   NaN   \n",
       "9                                   NaN                                   NaN   \n",
       "\n",
       "   string_statistics.distinct_count  \\\n",
       "0                               NaN   \n",
       "1                               NaN   \n",
       "2                               NaN   \n",
       "3                               NaN   \n",
       "4                               NaN   \n",
       "5                               NaN   \n",
       "6                               NaN   \n",
       "7                               NaN   \n",
       "8                               NaN   \n",
       "9                               NaN   \n",
       "\n",
       "  string_statistics.distribution.categorical.buckets  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "5                                                NaN  \n",
       "6                                                NaN  \n",
       "7                                                NaN  \n",
       "8                                                NaN  \n",
       "9                                                NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "      <th>string_constraints.domains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tGravityAcc-energy()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tGravityAcc-min()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tGravityAcc-max()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tGravityAcc-max()-Y</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tGravityAcc-min()-Y</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>angle(X,gravityMean)</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tGravityAcc-mean()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tGravityAcc-mean()-Y</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tBodyAccMag-max()</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tBodyGyroJerk-entropy()-X</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name inferred_type  completeness  \\\n",
       "0     tGravityAcc-energy()-X    Fractional           1.0   \n",
       "1        tGravityAcc-min()-X    Fractional           1.0   \n",
       "2        tGravityAcc-max()-X    Fractional           1.0   \n",
       "3        tGravityAcc-max()-Y    Fractional           1.0   \n",
       "4        tGravityAcc-min()-Y    Fractional           1.0   \n",
       "5       angle(X,gravityMean)    Fractional           1.0   \n",
       "6       tGravityAcc-mean()-X    Fractional           1.0   \n",
       "7       tGravityAcc-mean()-Y    Fractional           1.0   \n",
       "8          tBodyAccMag-max()    Fractional           1.0   \n",
       "9  tBodyGyroJerk-entropy()-X    Fractional           1.0   \n",
       "\n",
       "  num_constraints.is_non_negative string_constraints.domains  \n",
       "0                           False                        NaN  \n",
       "1                           False                        NaN  \n",
       "2                           False                        NaN  \n",
       "3                           False                        NaN  \n",
       "4                           False                        NaN  \n",
       "5                           False                        NaN  \n",
       "6                           False                        NaN  \n",
       "7                           False                        NaN  \n",
       "8                           False                        NaN  \n",
       "9                           False                        NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze collected data for data quality issues\n",
    "\n",
    "When you have collected the data above, analyze and monitor the data with Monitoring Schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "# Upload some test scripts to the S3 bucket for pre- and post-processing\n",
    "bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "bucket.Object(code_prefix + \"/preprocessor.py\").upload_file(\"preprocessor.py\")\n",
    "bucket.Object(code_prefix + \"/postprocessor.py\").upload_file(\"postprocessor.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a model monitoring schedule for the endpoint created earlier. Use the baseline resources (constraints and statistics) to compare against the realtime traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from sagemaker.model_monitor import BatchTransformInput\n",
    "from sagemaker.model_monitor import MonitoringDatasetFormat\n",
    "from time import gmtime, strftime\n",
    "\n",
    "statistics_path = \"{}/statistics.json\".format(baseline_results_uri)\n",
    "constraints_path = \"{}/constraints.json\".format(baseline_results_uri)\n",
    "\n",
    "mon_schedule_name = \"Xgboost-Mlops-model-monitor-schedule-\" + strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", gmtime()\n",
    ")\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    # record_preprocessor_script=pre_processor_script,\n",
    "    post_analytics_processor_script=s3_code_postprocessor_uri,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start generating some artificial traffic\n",
    "The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and inspect the schedule\n",
    "Once you describe, observe that the MonitoringScheduleStatus changes to Scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List executions\n",
    "The schedule starts jobs at the previously specified intervals. Here, you list the latest five executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait until you cross the hour boundary (in UTC) to see executions kick off. The code below has the logic for waiting.\n",
    "\n",
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing in the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "We created a hourly schedule above that begins executions ON the hour (plus 0-20 min buffer.\n",
      "We will have to wait till we hit the hour...\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: Xgboost-Mlops-model-monitor-schedule-2023-09-25-09-33-59\n",
      "Waiting for the first execution to happen...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above that begins executions ON the hour (plus 0-20 min buffer.\\nWe will have to wait till we hit the hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the first execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect a specific execution (latest execution)\n",
    "In the previous cell, you picked up the latest completed or failed scheduled execution. Here are the possible terminal states and what each of them mean: \n",
    "* `Completed` - The monitoring execution completed and no issues were found in the violations report.\n",
    "* `CompletedWithViolations` - The execution completed, but constraint violations were detected.\n",
    "* `Failed` - The monitoring execution failed, maybe due to client error (perhaps incorrect role premissions) or infrastructure issues. Further examination of `FailureReason` and `ExitMessage` is necessary to identify what exactly happened.\n",
    "* `Stopped` - The job exceeded max runtime or was manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1]  # Latest execution's index is -1, second to last is -2, etc\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_uri = latest_execution.output.destination\n",
    "print(\"Report Uri: {}\".format(report_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "print(\"Report bucket: {}\".format(report_bucket))\n",
    "print(\"Report key: {}\".format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violations report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any violations compared to the baseline are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other commands\n",
    "We can also start and stop the monitoring schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete resources\n",
    "\n",
    "You can keep your endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocations. That data persists in Amazon S3 until you delete it yourself.\n",
    "\n",
    "You need to delete the schedule before deleting the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.stop_monitoring_schedule()\n",
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(60)  # Wait for the deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/sagemaker_model_monitor|introduction|SageMaker-ModelMonitoring.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
